{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import ast\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Optional\n",
    "import builtins\n",
    "from lshashpy3 import LSHash\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"your API key\" \n",
    ")\n",
    "\n",
    "pre_prompt = (\"Please classify the following records into a two-dimensional list. Each element of the array \"\n",
    "              \"should be a group, containing the record IDs of that group (e.g., 1, 2, 3, etc.). Ensure that \"\n",
    "              \"each record ID is classified exactly once and appear once in the 2D array, without any \"\n",
    "              \"duplication or omission.The output should be a two-dimensional list with no additional information!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_ground_truth(file_path):\n",
    "    class UnionFind:\n",
    "            def __init__(self):\n",
    "                self.parent = {}\n",
    "\n",
    "            def find(self, x):\n",
    "                if self.parent[x] != x:\n",
    "                    self.parent[x] = self.find(self.parent[x])\n",
    "                return self.parent[x]\n",
    "\n",
    "            def union(self, x, y):\n",
    "                rootX = self.find(x)\n",
    "                rootY = self.find(y)\n",
    "                if rootX != rootY:\n",
    "                    self.parent[rootY] = rootX\n",
    "\n",
    "            def add(self, x):\n",
    "                if x not in self.parent:\n",
    "                    self.parent[x] = x\n",
    "\n",
    "    def merge_coordinates(coordinates):\n",
    "            uf = UnionFind()\n",
    "            ids = set()\n",
    "\n",
    "            for ltable_id, rtable_id in coordinates:\n",
    "                uf.add(ltable_id)\n",
    "                uf.add(rtable_id)\n",
    "                uf.union(ltable_id, rtable_id)\n",
    "                ids.add(ltable_id)\n",
    "                ids.add(rtable_id)\n",
    "\n",
    "            entity_groups = {}\n",
    "            for _id in ids:\n",
    "                root = uf.find(_id)\n",
    "                if root not in entity_groups:\n",
    "                    entity_groups[root] = []\n",
    "                entity_groups[root].append(_id)\n",
    "\n",
    "            result_1 = []\n",
    "            for root, records in entity_groups.items():\n",
    "                result_1.append(records)\n",
    "\n",
    "            return result_1\n",
    "    data = []\n",
    "    with open(file_path, newline='', encoding='MacRoman') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)  \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "\n",
    "    ground_truth_1 = merge_coordinates(data)\n",
    "    print(len(ground_truth_1))\n",
    "    ground_truth_new = []\n",
    "    for row in ground_truth_1:\n",
    "        tmp = []\n",
    "        for ids in row:\n",
    "            tmp.append(int(ids))\n",
    "        ground_truth_new.append(tmp)\n",
    "    return ground_truth_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(id_list, file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "        for r_id in id_list:\n",
    "            for row in rows:\n",
    "                if row['ID'] == str(r_id): \n",
    "                    lines.append(','.join([str(row[key]) for key in reader.fieldnames if key != 'ID']))\n",
    "                    break\n",
    "    prompts = '\\n'.join(lines)\n",
    "    return prompts\n",
    "\n",
    "def vectorize_data(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(text.split('\\n'))  \n",
    "    return embeddings\n",
    "\n",
    "def elbow_method(embeddings, max_k=5):\n",
    "    if embeddings is None or embeddings.shape[0] < 2:\n",
    "        return 1  \n",
    "    \n",
    "    distortions = []\n",
    "    K = range(1, min(max_k, embeddings.shape[0]) + 1)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "        kmeans.fit(embeddings)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    optimal_k_index = np.argmin(distortions[1:]) + 1  \n",
    "    optimal_k = K[optimal_k_index]\n",
    "\n",
    "    print(f\"best is : {optimal_k}\")\n",
    "    return optimal_k\n",
    "\n",
    "def kmeans_clustering(embeddings, n_clusters):\n",
    "    if embeddings is None or len(embeddings) < n_clusters:\n",
    "        return np.zeros(len(embeddings), dtype=int)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42,n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def format_output(id_list, labels):\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(id_list[i])\n",
    "    \n",
    "    sorted_clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    output = []\n",
    "    for cluster in sorted_clusters:\n",
    "        output.append([int(item) for item in cluster[1]])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def read_csv_to_2d_array(file_path):\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    return data\n",
    "\n",
    "def get_prompt_from_ids(id_list, file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='MacRoman') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "        for r_id in id_list:\n",
    "            for row in rows:\n",
    "                if row['ID'] == str(r_id): \n",
    "                    rec_str = f\"Record {r_id}: \"\n",
    "                    rec_str += ','.join([str(row[key]) for key in reader.fieldnames if key != 'ID'])\n",
    "                    lines.append(rec_str)\n",
    "                    break\n",
    "    return '\\n'.join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_sampling(original_array):\n",
    "    result = []\n",
    "    row_indices = {i: row.copy() for i, row in enumerate(original_array)}\n",
    "    total_ids = sum(len(row) for row in original_array)\n",
    "    \n",
    "    while True:\n",
    "        group = []\n",
    "        while len(group) < 10:\n",
    "            flag = False\n",
    "            for i in range(len(original_array)):\n",
    "                if row_indices[i]: \n",
    "                    group.append(row_indices[i].pop(0))\n",
    "                    flag = True\n",
    "                if len(group) == 10:  \n",
    "                    break\n",
    "            if not flag: \n",
    "                break\n",
    "        if group:\n",
    "            result.append(group)\n",
    "        if not any(row_indices.values()):\n",
    "            break\n",
    "    \n",
    "    output_ids = [id for group in result for id in group]\n",
    "    if len(output_ids) != total_ids:\n",
    "        raise ValueError(\"wrong number!\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_sampled_ids(csv_file,sample_ids_list):\n",
    "    execution_time=0\n",
    "    use_number = 0\n",
    "    total_tokens_call = 0\n",
    "    \n",
    "    all_classified_results = []\n",
    "    for ids in sample_ids_list:\n",
    "        content_prompt = get_prompt_from_ids(id_list = ids, file_path = csv_file)\n",
    "        start_time = time.time()\n",
    "        completion = client.chat.completions.create(\n",
    "                            model = \"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a worker with rich experience performing Entity Resolution tasks. You specialize in clustering and classification within ER.\"},\n",
    "                            {\"role\": \"user\", \"content\": pre_prompt + content_prompt},\n",
    "                            ]\n",
    "                        )\n",
    "        execution_time += (time.time() - start_time)\n",
    "        use_number += 1\n",
    "        token_number = completion.usage.total_tokens\n",
    "        total_tokens_call += token_number\n",
    "        content = completion.choices[0].message.content\n",
    "        content = content.replace('\\n', '').replace(' ', '')\n",
    "        content_cleaned = re.sub(r\"[^\\d\\[\\],]\", \"\", content)\n",
    "        content_cleaned = re.sub(r\",\\s*]\", \"]\", content_cleaned)\n",
    "        content_cleaned = re.sub(r\",+\", \",\", content_cleaned)\n",
    "        matches = re.findall(r'\\[([^\\[\\]]*?)\\]', content_cleaned)\n",
    "        result_llm = []\n",
    "        import builtins\n",
    "        for match in matches:\n",
    "            match_cleaned = match.strip()\n",
    "            if ',' in match_cleaned:\n",
    "                sublist = [int(num) for num in match_cleaned.split(',')]\n",
    "                result_llm.append(sublist)\n",
    "            else:\n",
    "                result_llm.append([int(num) for num in match_cleaned.split()])\n",
    "        all_classified_results.append(result_llm) \n",
    "    return all_classified_results,execution_time,use_number,total_tokens_call\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def the_most_importent_one(vector_data,classified_results):\n",
    "    result_for_find = []\n",
    "    for classified_results_row in classified_results:\n",
    "        list_select = []\n",
    "        vectored_select = []\n",
    "        for cluster_row in classified_results_row:\n",
    "            vectored_select = np.array([vector_data[id_] for id_ in cluster_row])\n",
    "            avg_vector = np.mean(vectored_select, axis=0) \n",
    "            distances = [np.linalg.norm(vectored_select[i] - avg_vector) for i in range(len(cluster_row))]\n",
    "            representative_id = cluster_row[np.argmin(distances)]\n",
    "            list_select.append(representative_id)\n",
    "        result_for_find.append(list_select)\n",
    "    return result_for_find\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_2d_array_from_file(file_path):\n",
    "    array_list = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                row = list(map(int, line.strip().split()))\n",
    "                array_list.append(row)\n",
    "        return array_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path}  is not found !\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Exist wrong number ï¼š{e}\")\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def the_most_importent_one_1(classified_results):\n",
    "    result_for_find = []\n",
    "    for classified_results_row in classified_results:\n",
    "        list_select = []\n",
    "        for cluster_row in classified_results_row:   \n",
    "            representative_id = cluster_row[0]\n",
    "            list_select.append(representative_id)\n",
    "        result_for_find.append(list_select)\n",
    "    return result_for_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "def find_most_similar(\n",
    "    current_id: Optional[int], \n",
    "    candidate_ids: List[int], \n",
    "    similarity_matrix: List[List[float]]\n",
    ") -> Optional[int]:\n",
    "\n",
    "    if not candidate_ids:\n",
    "        return None\n",
    "    \n",
    "    if current_id is None:\n",
    "        return candidate_ids[0]\n",
    "\n",
    "    max_similarity = -float('inf')\n",
    "    most_similar_id = None\n",
    "    \n",
    "    for candidate_id in candidate_ids:\n",
    "        similarity = similarity_matrix[current_id][candidate_id]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_id = candidate_id\n",
    "    \n",
    "    return most_similar_id\n",
    "\n",
    "def filter_available_ids(next_row_ids: List[int], id_assigned: set) -> List[int]:\n",
    "    return [id_ for id_ in next_row_ids if id_ not in id_assigned]\n",
    "\n",
    "def process_rounds(\n",
    "    id_matrix: List[List[int]],\n",
    "    similarity_matrix: List[List[float]],\n",
    "    max_length: int\n",
    ") -> List[List[int]]:\n",
    "    all_rounds = []\n",
    "    id_assigned = set()\n",
    "    total_rows = len(id_matrix)\n",
    "\n",
    "    while True:\n",
    "        current_round = []\n",
    "        current_id = None\n",
    "\n",
    "        for row_index in range(total_rows):\n",
    "            available_ids = filter_available_ids(id_matrix[row_index], id_assigned)\n",
    "            if not available_ids:\n",
    "                continue\n",
    "\n",
    "            next_id = find_most_similar(current_id, available_ids, similarity_matrix)\n",
    "            if next_id is not None:\n",
    "                current_round.append(next_id)\n",
    "                id_assigned.add(next_id)\n",
    "                current_id = next_id\n",
    "        if not current_round:\n",
    "            break\n",
    "        \n",
    "        all_rounds.append(current_round)\n",
    "    \n",
    "    return all_rounds\n",
    "\n",
    "\n",
    "\n",
    "def traverse_ids_to_2d(\n",
    "    id_matrix: List[List[int]], \n",
    "    similarity_matrix: List[List[float]],\n",
    "    max_length: int = 10, \n",
    "    batch_size: int = 10\n",
    ") -> List[List[int]]:\n",
    "    all_rounds = process_rounds(id_matrix, similarity_matrix, max_length)\n",
    "    return all_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_find(x, pa):\n",
    "    if pa[x] != x:\n",
    "        pa[x] = merge_find(pa[x], pa)  \n",
    "    return pa[x]\n",
    "\n",
    "\n",
    "def merge_union(x, y, pa):\n",
    "    rootX = merge_find(x, pa)\n",
    "    rootY = merge_find(y, pa)\n",
    "    if rootX != rootY:\n",
    "        pa[rootY] = rootX  \n",
    "\n",
    "\n",
    "def find_simi_nex(small_clusters, now_cluster , pa , ini_simi,the_max_nex):\n",
    "    global i1, i2\n",
    "    pattern = [] \n",
    "    for i in range(len(now_cluster) - 1):\n",
    "        for j in range(i + 1, len(now_cluster)):\n",
    "            if ini_simi[now_cluster[i]][now_cluster[j]] >= the_max_nex:\n",
    "                # print(maper[now_cluster[i]], maper[now_cluster[j]])\n",
    "                # print([now_cluster[i], now_cluster[j]])\n",
    "                pattern.append([now_cluster[i], now_cluster[j]])\n",
    "    for x, y in pattern:\n",
    "        for i in range(len(small_clusters)):\n",
    "            if x in small_clusters[i]:\n",
    "                i1 = i\n",
    "                break\n",
    "        for i in range(len(small_clusters)):\n",
    "            if y in small_clusters[i]:\n",
    "                i2 = i\n",
    "                break\n",
    "        merge_union(i1, i2, pa)\n",
    "    merged_groups = {}\n",
    "    for i in range(len(small_clusters)):\n",
    "        root = merge_find(i, pa)\n",
    "        if root not in merged_groups:\n",
    "            merged_groups[root] = []\n",
    "        merged_groups[root].extend(small_clusters[i])\n",
    "    result = [sorted(set(values)) for values in merged_groups.values()]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import combinations\n",
    "\n",
    "def is_satisfied(result_matrix, similarity_matrix):\n",
    "    sim_matrix = np.array(similarity_matrix)\n",
    "    \n",
    "    min_intra = float('inf')\n",
    "    for group in result_matrix:\n",
    "        if len(group) < 2:\n",
    "            continue  \n",
    "        pairs = np.array([(i, j) for i in group for j in group if i < j])\n",
    "        if len(pairs) == 0:\n",
    "            continue\n",
    "        sims = sim_matrix[pairs[:, 0], pairs[:, 1]]\n",
    "        group_min = np.min(sims)\n",
    "        if group_min < min_intra:\n",
    "            min_intra = group_min\n",
    "    \n",
    "    max_inter = float('-inf')\n",
    "    \n",
    "    group_sets = [set(group) for group in result_matrix]\n",
    "    group_pairs = list(combinations(range(len(result_matrix)), 2))\n",
    "    \n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_results = []\n",
    "        for i, j in group_pairs:\n",
    "            future = executor.submit(\n",
    "                compute_max_inter_similarity, \n",
    "                group_sets[i], group_sets[j], sim_matrix\n",
    "            )\n",
    "            future_results.append(future)\n",
    "        \n",
    "        for future in future_results:\n",
    "            group_pair_max = future.result()\n",
    "            if group_pair_max > max_inter:\n",
    "                max_inter = group_pair_max\n",
    "    return min_intra >= max_inter\n",
    "\n",
    "def compute_max_inter_similarity(group_i, group_j, sim_matrix):\n",
    "    max_sim = float('-inf')\n",
    "    for i in group_i:\n",
    "        for j in group_j:\n",
    "            sim = sim_matrix[i, j]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_handle_missing_ids(one_slice, result_tmp, attempt):\n",
    "    detected_ids = set()\n",
    "    for group in result_tmp:\n",
    "        detected_ids.update(group)\n",
    "    missing_ids = set(one_slice) - detected_ids\n",
    "    \n",
    "    if not missing_ids:\n",
    "        return result_tmp, True\n",
    "    else:\n",
    "        if attempt == 0:\n",
    "            print(f\"Retrying...\")\n",
    "        else:\n",
    "            for missing_id in missing_ids:\n",
    "                result_tmp.append([missing_id])\n",
    "        return result_tmp, False\n",
    "\n",
    "\n",
    "def llm_seperate(data_list, data_file, ini_simi, the_max_nex):\n",
    "    api_call_time = 0\n",
    "    use_time = 0\n",
    "    use_token = 0\n",
    "    seperate_input_token = 0\n",
    "    seperate_output_token = 0\n",
    "    result_sliced = []\n",
    "    number = math.ceil(len(data_list) / 10)\n",
    "    sliced_lists = [data_list[i * 10:(i + 1) * 10] for i in range(number)]\n",
    "\n",
    "    for one_slice in sliced_lists:\n",
    "        api_call_time += 1\n",
    "        for attempt in range(2):  \n",
    "            start_time = time.time()\n",
    "            prompt_sliced = get_prompt_from_ids(one_slice, data_file)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": \"You are a worker specialize in clustering and classification within Entity Resolution.\"},\n",
    "                    {\"role\": \"user\", \"content\": pre_prompt + prompt_sliced},\n",
    "                ]\n",
    "            )\n",
    "            use_time += time.time() - start_time\n",
    "            prompt_tokens = completion.usage.prompt_tokens  \n",
    "            seperate_input_token += prompt_tokens\n",
    "            completion_tokens = completion.usage.completion_tokens \n",
    "            seperate_output_token += completion_tokens\n",
    "            token_number = completion.usage.total_tokens\n",
    "            use_token += token_number\n",
    "            content = completion.choices[0].message.content\n",
    "            content = content.replace('\\n', '').replace(' ', '')\n",
    "            content_cleaned = re.sub(r\"[^\\d\\[\\],]\", \"\", content)\n",
    "            content_cleaned = re.sub(r\",\\s*]\", \"]\", content_cleaned)\n",
    "            content_cleaned = re.sub(r\",+\", \",\", content_cleaned)\n",
    "            matches = re.findall(r'\\[([^\\[\\]]*?)\\]', content_cleaned)\n",
    "            result_tmp = []\n",
    "            for match in matches:\n",
    "                match_cleaned = match.strip()\n",
    "                if ',' in match_cleaned:\n",
    "                    sublist = [int(num) for num in match_cleaned.split(',')]\n",
    "                    result_tmp.append(sublist)\n",
    "                else:\n",
    "                    result_tmp.append([int(num) for num in match_cleaned.split()])\n",
    "\n",
    "            result_tmp, complete = check_and_handle_missing_ids(one_slice, result_tmp, attempt)\n",
    "            if complete:\n",
    "                break\n",
    "\n",
    "        for row_slice in result_tmp:\n",
    "            result_sliced.append(row_slice)\n",
    "    parent = list(range(len(result_sliced)))\n",
    "    array_new = find_simi_nex(result_sliced, data_list, parent, ini_simi, the_max_nex)\n",
    "    return array_new, api_call_time, use_time, use_token, seperate_input_token, seperate_output_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_back(two_d_array, three_d_array):\n",
    "\n",
    "    num_to_row = {}\n",
    "    for matrix in three_d_array:\n",
    "        for row in matrix:\n",
    "            for number in row:\n",
    "                if number not in num_to_row:\n",
    "                    num_to_row[number] = row\n",
    "    for i, row in enumerate(two_d_array):\n",
    "        new_row = []\n",
    "        for number in row:\n",
    "            if number in num_to_row:\n",
    "                new_row.extend(num_to_row[number])\n",
    "        two_d_array[i] = list(dict.fromkeys(new_row))\n",
    "    return two_d_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_total_simi_vector(data_file_path,model_file):\n",
    "    model = SentenceTransformer(model_file)\n",
    "    def combine_attributes(row):\n",
    "        return ' '.join(str(value) for value in row[1:]) \n",
    "    data = pd.read_csv(data_file_path,encoding=\"MacRoman\")\n",
    "    data['combined_text'] = data.apply(combine_attributes, axis=1)\n",
    "    vectors = data['combined_text'].apply(lambda text: model.encode(text)).tolist() \n",
    "    simi_matrix = cosine_similarity(vectors)\n",
    "    print(\"calculate similarity matrix done\")\n",
    "    return vectors,simi_matrix,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def bipartite_clustering(data, similarity_matrix):\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        G.add_node(i)\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        for j in range(len(similarity_matrix)):\n",
    "            if similarity_matrix[i][j] > 0:\n",
    "                G.add_edge(i, j)\n",
    "    node_partition = nx.bipartite.color(G)\n",
    "    cluster1 = []\n",
    "    cluster2 = []\n",
    "    for i, color in node_partition.items():\n",
    "        if color == 0:\n",
    "            cluster1.append(data[i])\n",
    "        else:\n",
    "            cluster2.append(data[i])\n",
    "\n",
    "    return [cluster1, cluster2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def read_clusters_from_csv(filename):\n",
    "    clusters = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            clusters.append([int(item) for item in row if item])  \n",
    "    return clusters\n",
    "\n",
    "def calculate_purity(true_clusters, predicted_clusters):\n",
    "    total_samples = sum(len(cluster) for cluster in predicted_clusters)\n",
    "    total_correct = 0\n",
    "\n",
    "    for pred_cluster in predicted_clusters:\n",
    "        label_count = Counter()\n",
    "        for sample in pred_cluster:\n",
    "            for true_cluster in true_clusters:\n",
    "                if sample in true_cluster:\n",
    "                    label_count[tuple(true_cluster)] += 1\n",
    "        if label_count:\n",
    "            max_label_count = max(label_count.values())\n",
    "            total_correct += max_label_count\n",
    "\n",
    "    return total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "def calculate_inverse_purity(true_clusters, predicted_clusters):\n",
    "    total_samples = sum(len(cluster) for cluster in true_clusters)\n",
    "    total_correct = 0\n",
    "\n",
    "    for true_cluster in true_clusters:\n",
    "        if true_cluster:\n",
    "            pred_labels = Counter()\n",
    "            for sample in true_cluster:\n",
    "                for pred_cluster in predicted_clusters:\n",
    "                    if sample in pred_cluster:\n",
    "                        pred_labels[tuple(pred_cluster)] += 1\n",
    "            if pred_labels:\n",
    "                max_match = max(pred_labels.values())\n",
    "                total_correct += max_match\n",
    "\n",
    "    return total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "def calculate_fp_measure(true_clusters, predicted_clusters):\n",
    "    purity = calculate_purity(true_clusters, predicted_clusters)\n",
    "    inverse_purity = calculate_inverse_purity(true_clusters, predicted_clusters)\n",
    "\n",
    "    if purity + inverse_purity == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (purity * inverse_purity) / (purity + inverse_purity)\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def convert_to_labels(clusters, n_samples):\n",
    "    labels = [-1] * n_samples \n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for sample in cluster:\n",
    "            labels[sample] = cluster_id\n",
    "    return labels\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "def calculate_ari(true_clusters, predicted_clusters):\n",
    "    all_samples = set(sample for cluster in true_clusters for sample in cluster) | \\\n",
    "    set(sample for cluster in predicted_clusters for sample in cluster)\n",
    "    n_samples = max(all_samples) + 1 \n",
    "    true_labels = convert_to_labels(true_clusters, n_samples)\n",
    "    predicted_labels = convert_to_labels(predicted_clusters, n_samples)\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    return ari\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate(vectors,simi_matrix,merge_clusters_pre,data_file_path,the_max_nex):\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    seperate_input = 0\n",
    "    seperate_output = 0\n",
    "    sperate_result = []\n",
    "    for id_list in merge_clusters_pre:\n",
    "        print(id_list)\n",
    "        text_data = get_data(id_list, data_file_path)\n",
    "        vectorized_data = vectorize_data(text_data)\n",
    "        n_clusters = elbow_method(vectorized_data) \n",
    "        labels = kmeans_clustering(vectorized_data, n_clusters)\n",
    "        clusters_labels = format_output(id_list, labels)\n",
    "        prompt_id = dynamic_sampling(clusters_labels)\n",
    "        classified_results, execute_time , use_number , total_tokens = process_sampled_ids(data_file_path, prompt_id)\n",
    "        sperate_time+=execute_time\n",
    "        api_call_time_all+=use_number\n",
    "        sperate_token+=total_tokens\n",
    "        result_for_found = the_most_importent_one(vectors,classified_results) \n",
    "        target_list = traverse_ids_to_2d(result_for_found, simi_matrix, max_length=10, batch_size=10) \n",
    "        llm_tmp = []\n",
    "        for row_slice in target_list:\n",
    "            print(row_slice)\n",
    "            array_new,api_call_time,use_time,use_token , seperate_input_token , seperate_output_token = llm_seperate(row_slice,data_file_path,simi_matrix,the_max_nex)\n",
    "            api_call_time_all +=api_call_time\n",
    "            sperate_time += use_time\n",
    "            sperate_token += use_token\n",
    "            seperate_input += seperate_input_token\n",
    "            seperate_output += seperate_output_token\n",
    "            llm_tmp = llm_tmp + array_new\n",
    "        find_back_matrix = find_back(llm_tmp,classified_results)\n",
    "        sperate_result += find_back_matrix\n",
    "    print(\"seperate done\")\n",
    "    return sperate_result, api_call_time_all ,sperate_time, sperate_token, seperate_input , seperate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_jac(simi_matrix,merge_clusters_pre,data_file_path,the_max_nex):\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    seperate_input = 0\n",
    "    seperate_output = 0\n",
    "    sperate_result = []\n",
    "    for id_list in merge_clusters_pre:\n",
    "        print(id_list)\n",
    "        text_data = get_data(id_list, data_file_path)\n",
    "        vectorized_data = vectorize_data(text_data)\n",
    "        n_clusters = elbow_method(vectorized_data) \n",
    "        labels = kmeans_clustering(vectorized_data, n_clusters)\n",
    "        clusters_labels = format_output(id_list, labels)\n",
    "        prompt_id = dynamic_sampling(clusters_labels)\n",
    "        classified_results, execute_time , use_number , total_tokens = process_sampled_ids(data_file_path, prompt_id)\n",
    "        sperate_time+=execute_time\n",
    "        api_call_time_all+=use_number\n",
    "        sperate_token+=total_tokens\n",
    "\n",
    "        result_for_found = the_most_importent_one_1(classified_results) \n",
    "        target_list = traverse_ids_to_2d(result_for_found, simi_matrix, max_length=10, batch_size=10) \n",
    "        llm_tmp = []\n",
    "        for row_slice in target_list:\n",
    "            print(row_slice)\n",
    "            array_new,api_call_time,use_time,use_token , seperate_input_token , seperate_output_token = llm_seperate(row_slice,data_file_path,simi_matrix,the_max_nex)\n",
    "            api_call_time_all +=api_call_time\n",
    "            sperate_time += use_time\n",
    "            sperate_token += use_token\n",
    "            seperate_input += seperate_input_token\n",
    "            seperate_output += seperate_output_token\n",
    "            llm_tmp = llm_tmp + array_new\n",
    "        find_back_matrix = find_back(llm_tmp,classified_results)\n",
    "        sperate_result += find_back_matrix\n",
    "    print(\"seperate done\")\n",
    "    return sperate_result, api_call_time_all ,sperate_time, sperate_token, seperate_input , seperate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id_list(id_list, vectors, simi_matrix, data_file_path):\n",
    "    text_data = get_data(id_list, data_file_path)\n",
    "    vectorized_data = vectorize_data(text_data)\n",
    "    n_clusters = elbow_method(vectorized_data) \n",
    "    # n_clusters = 2\n",
    "    labels = kmeans_clustering(vectorized_data, n_clusters)\n",
    "    clusters_labels = format_output(id_list, labels)\n",
    "\n",
    "    prompt_id = dynamic_sampling(clusters_labels)\n",
    "    classified_results, execute_time, use_number, total_tokens = process_sampled_ids(data_file_path, prompt_id)\n",
    "\n",
    "    result_for_found = the_most_importent_one(vectors, classified_results)\n",
    "    target_list = traverse_ids_to_2d(result_for_found, simi_matrix, max_length=10, batch_size=10)\n",
    "\n",
    "    llm_tmp = []\n",
    "    api_call_time_all = use_number\n",
    "    sperate_time = execute_time\n",
    "    sperate_token = total_tokens\n",
    "\n",
    "    for row_slice in target_list:\n",
    "        array_new, api_call_time, use_time, use_token = llm_seperate(row_slice, data_file_path, simi_matrix)\n",
    "        api_call_time_all += api_call_time\n",
    "        sperate_time += use_time\n",
    "        sperate_token += use_token\n",
    "        llm_tmp.extend(array_new)\n",
    "\n",
    "    find_back_matrix = find_back(llm_tmp, classified_results)\n",
    "    return find_back_matrix, api_call_time_all, sperate_time, sperate_token\n",
    "\n",
    "def seperate_2(vectors, simi_matrix, merge_clusters_pre, data_file_path):\n",
    "    sperate_result = []\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_id_list, id_list, vectors, simi_matrix, data_file_path) for id_list in merge_clusters_pre]\n",
    "\n",
    "        for future in futures:\n",
    "            result, api_call_time, sperate_time_each, sperate_token_each = future.result()\n",
    "            sperate_result.extend(result)\n",
    "            api_call_time_all += api_call_time\n",
    "            sperate_time += sperate_time_each\n",
    "            sperate_token += sperate_token_each\n",
    "\n",
    "    print(\"separate done\")\n",
    "    return sperate_result, api_call_time_all, sperate_time, sperate_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_id_list(id_list, simi_matrix, data_file_path,the_threhold):\n",
    "\n",
    "    llm_tmp = []\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    seperate_token_input = 0\n",
    "    seperate_token_output = 0\n",
    "    array_new, api_call_time, use_time, use_token,A,B = llm_seperate(id_list, data_file_path, simi_matrix, the_threhold)\n",
    "    api_call_time_all += api_call_time\n",
    "    sperate_time += use_time\n",
    "    sperate_token += use_token\n",
    "    seperate_token_input += A\n",
    "    seperate_token_output += B\n",
    "    for row in array_new:\n",
    "        llm_tmp.append(row)\n",
    "\n",
    "\n",
    "    return llm_tmp, api_call_time_all, sperate_time, sperate_token,seperate_token_input,seperate_token_output\n",
    "\n",
    "def seperate_4(simi_matrix, merge_clusters_pre, data_file_path,the_threhold):\n",
    "    sperate_result = []\n",
    "    api_call_time_all = 0\n",
    "    sperate_time = 0\n",
    "    sperate_token = 0\n",
    "    seperate_input_token = 0\n",
    "    seperate_out_token = 0\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_id_list, id_list,  simi_matrix, data_file_path,the_threhold) for id_list in merge_clusters_pre]\n",
    "\n",
    "        for future in futures:\n",
    "            result, api_call_time, sperate_time_each, sperate_token_each , input_token,output_token = future.result()\n",
    "            sperate_result.extend(result)\n",
    "            api_call_time_all += api_call_time\n",
    "            sperate_time += sperate_time_each\n",
    "            sperate_token += sperate_token_each\n",
    "            seperate_input_token += input_token\n",
    "            seperate_out_token += output_token\n",
    "    print(\"separate done\")\n",
    "    return sperate_result, api_call_time_all, sperate_time, sperate_token,seperate_input_token,seperate_out_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_block(vectors,data,similarity_threshold):\n",
    "    lsh = LSHash(hash_size=15, input_dim=384, num_hashtables=8)\n",
    "    for ix,vec in enumerate(vectors):\n",
    "        lsh.index(vec,extra_data=ix)\n",
    "    graph = defaultdict(set)\n",
    "    for ix, vec in enumerate(vectors):\n",
    "        results = lsh.query(vec, num_results=None, distance_func='cosine')\n",
    "        for res in results:\n",
    "            if res[0][1] is not None:\n",
    "                jx = res[0][1]\n",
    "                if jx != ix:\n",
    "                    similarity = 1 - res[1]\n",
    "                    if similarity > similarity_threshold:\n",
    "                        graph[ix].add(jx)\n",
    "                        graph[jx].add(ix)\n",
    "    def find_connected_components(graph):\n",
    "        visited = set()\n",
    "        components = []\n",
    "\n",
    "        def dfs(node, component):\n",
    "            stack = [node]\n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if current not in visited:\n",
    "                    visited.add(current)\n",
    "                    component.append(current)\n",
    "                    for neighbor in graph[current]:\n",
    "                        if neighbor not in visited:\n",
    "                            stack.append(neighbor)\n",
    "\n",
    "        for node in range(len(data)):\n",
    "            if node not in visited:\n",
    "                component = []\n",
    "                dfs(node, component)\n",
    "                components.append(component)\n",
    "        return components\n",
    "    components = find_connected_components(graph)\n",
    "    clusters_array = []\n",
    "    clusters = []\n",
    "    for component in components:\n",
    "        valid_indices = [idx for idx in component if idx is not None and 0 <= idx < len(data)]\n",
    "        cluster = data.iloc[valid_indices, 0].tolist()  \n",
    "        clusters.append(cluster)\n",
    "        clusters_array.append(valid_indices)\n",
    "    print(\"lsh done\")\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_elements(list1, list2, n=2):\n",
    "    combined_length = len(list1) + len(list2)\n",
    "    \n",
    "    if combined_length <= n:\n",
    "        return list1 + list2\n",
    "    \n",
    "    # Ensure at least one element from each list\n",
    "    if len(list1) == 0 or len(list2) == 0:\n",
    "        raise ValueError(\"Both lists must contain at least one element\")\n",
    "    result = [random.choice(list1), random.choice(list2)]\n",
    "    \n",
    "    remaining_slots = n - len(result)\n",
    "    combined_list = list1 + list2\n",
    "    combined_list.remove(result[0])\n",
    "    combined_list.remove(result[1])\n",
    "    \n",
    "    # Randomly choose the remaining elements\n",
    "    result += random.sample(combined_list, remaining_slots)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_most_simi(list1,list2,init_simi):\n",
    "    max_simi = 0\n",
    "    record_a = 0\n",
    "    record_b = 0\n",
    "    for a in list1:\n",
    "        for b in list2:\n",
    "            if init_simi[a][b]>max_simi:\n",
    "                max_simi = init_simi[a][b]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return max_simi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_act(a,b,batch_simi):\n",
    "    for i in range(a+1,b-1):\n",
    "        if batch_simi[a][i]==0 and batch_simi[i][b]==0:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_top_cells(similarity_matrix , xiaxian,shagnxian ,shuliang):\n",
    "    triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "    triu_values = similarity_matrix[triu_indices]\n",
    "    valid_indices = np.where((triu_values >= xiaxian) & (triu_values <=shagnxian))[0]\n",
    "    selected_indices = valid_indices[:shuliang]\n",
    "    result = [[int(triu_indices[0][i]), int(triu_indices[1][i])] for i in selected_indices]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_merge_cells(similarity_matrix , xiaxian,shagnxian):\n",
    "\n",
    "    triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "    triu_values = similarity_matrix[triu_indices]\n",
    "    valid_indices = np.where((triu_values >= xiaxian) & (triu_values <=shagnxian))[0]\n",
    "    result = [[int(triu_indices[0][i]), int(triu_indices[1][i])] for i in valid_indices]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_2(clusters,simi_matrix,file_path,block_threshold , merge_threshold):\n",
    "    pre_prompt_merge = (\"Do the records in the following clusters refer to the same entity? i.e., given that the records in each \"\n",
    "              \"cluster refer to the same entity, can these clusters or parts of these clusters be merged? If they all \"\n",
    "              \"point to one entity, answer 'Yes' And returns a two-dimensional array, each dimension of the array is \"\n",
    "              \"the cluster id, indicating which clusters can be clustered together, otherwise just answer 'No' with \"\n",
    "              \"no reason.You only need to tell me yes or no!!!\\n\")\n",
    "    class UnionFind:\n",
    "        def __init__(self):\n",
    "            self.parent = {}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            rootX = self.find(x)\n",
    "            rootY = self.find(y)\n",
    "            if rootX != rootY:\n",
    "                self.parent[rootY] = rootX\n",
    "\n",
    "        def add(self, x):\n",
    "            if x not in self.parent:\n",
    "                self.parent[x] = x\n",
    "\n",
    "    def merge_coordinates(coordinates):\n",
    "        uf = UnionFind()\n",
    "        ids = set()\n",
    "\n",
    "        for ltable_id, rtable_id in coordinates:\n",
    "            uf.add(ltable_id)\n",
    "            uf.add(rtable_id)\n",
    "            uf.union(ltable_id, rtable_id)\n",
    "            ids.add(ltable_id)\n",
    "            ids.add(rtable_id)\n",
    "\n",
    "        entity_groups = {}\n",
    "        for _id in ids:\n",
    "            root = uf.find(_id)\n",
    "            if root not in entity_groups:\n",
    "                entity_groups[root] = []\n",
    "            entity_groups[root].append(_id)\n",
    "\n",
    "        result_1 = []\n",
    "        for root, records in entity_groups.items():\n",
    "            result_1.append(records)\n",
    "\n",
    "        return result_1\n",
    "    api_use_time = 0\n",
    "    merge_time = 0\n",
    "    merge_token = 0\n",
    "    merge_input_token = 0\n",
    "    merge_output_token = 0\n",
    "    map_merge = [0]*len(clusters)\n",
    "    row_merge = len(clusters)\n",
    "    batch_simi = [[0] * row_merge for _ in range(row_merge)]\n",
    "    for i in range(row_merge):\n",
    "        for j in range(i,row_merge):\n",
    "            batch_simi[i][j] = get_most_simi(clusters[i],clusters[j],simi_matrix)\n",
    "    similarity_matrix = np.array(batch_simi)\n",
    "    need_merge = [] \n",
    "    for threshold in np.arange(merge_threshold+0.02 , block_threshold, 0.02):\n",
    "        print(threshold)\n",
    "        selected_target = find_top_cells(similarity_matrix, threshold-0.02, threshold, 10)\n",
    "        if len(selected_target) == 0:\n",
    "            continue\n",
    "        count_yes = 0\n",
    "        for row_selected in selected_target:\n",
    "            list_all = pick_elements(clusters[row_selected[0]],clusters[row_selected[1]])\n",
    "            prompt = get_prompt_from_ids(list_all, file_path)\n",
    "            start_time = time.time()\n",
    "            completion = client.chat.completions.create(\n",
    "                            model = \"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a worker with rich experience performing Entity Resolution tasks. You specialize in clustering and classification within ER.\"},\n",
    "                            {\"role\": \"user\", \"content\": pre_prompt_merge + prompt},\n",
    "                            ]\n",
    "                        )\n",
    "            merge_time += time.time() - start_time\n",
    "            api_use_time = api_use_time + 1\n",
    "            prompt_tokens = completion.usage.prompt_tokens  \n",
    "            merge_input_token += prompt_tokens\n",
    "            completion_tokens = completion.usage.completion_tokens  \n",
    "            merge_output_token += completion_tokens\n",
    "            token_number = completion.usage.total_tokens\n",
    "            merge_token += token_number\n",
    "            answer = completion.choices[0].message.content.lower().strip() \n",
    "            if 'yes' in answer:\n",
    "                count_yes += 1\n",
    "        if count_yes/len(selected_target) >= 0.2:\n",
    "            need_merge += find_merge_cells(similarity_matrix , threshold-0.02, threshold)\n",
    "    for row_in_need_merge in need_merge:\n",
    "        map_merge[row_in_need_merge[0]] = 1\n",
    "        map_merge[row_in_need_merge[1]] = 1 \n",
    "    map_rest = []\n",
    "    for i in range(len(clusters)):\n",
    "        if map_merge[i] == 0:\n",
    "\n",
    "            map_rest.append(clusters[i]) \n",
    "    new_result = []\n",
    "    result_1 = merge_coordinates(need_merge)\n",
    "    for row in result_1:\n",
    "        tmp = []\n",
    "        for ids in row:\n",
    "            tmp.extend(clusters[ids])\n",
    "        new_result.append(tmp)\n",
    "    map_rest += new_result\n",
    "    print(\"merge_done\")\n",
    "    return map_rest , api_use_time ,merge_time , merge_token , merge_input_token, merge_output_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(clusters,simi_matrix,file_path):\n",
    "    pre_prompt_merge = (\"Do the records in the following clusters refer to the same entity? i.e., given that the records in each \"\n",
    "              \"cluster refer to the same entity, can these clusters or parts of these clusters be merged? If they all \"\n",
    "              \"point to one entity, answer 'Yes' And returns a two-dimensional array, each dimension of the array is \"\n",
    "              \"the cluster id, indicating which clusters can be clustered together, otherwise just answer 'No' with \"\n",
    "              \"no reason.You only need to tell me yes or no!!!\\n\")\n",
    "    class UnionFind:\n",
    "        def __init__(self):\n",
    "            self.parent = {}\n",
    "            self.rank = {}\n",
    "            self.size = {}\n",
    "\n",
    "        def find(self, x):\n",
    "            if x not in self.parent:\n",
    "                self.parent[x] = x\n",
    "                self.rank[x] = 0\n",
    "                self.size[x] = 1\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            rootX = self.find(x)\n",
    "            rootY = self.find(y)\n",
    "\n",
    "            if rootX != rootY:\n",
    "                if self.rank[rootX] > self.rank[rootY]:\n",
    "                    self.parent[rootY] = rootX\n",
    "                    self.size[rootX] += self.size[rootY]\n",
    "                elif self.rank[rootX] < self.rank[rootY]:\n",
    "                    self.parent[rootX] = rootY\n",
    "                    self.size[rootY] += self.size[rootX]\n",
    "                else:\n",
    "                    self.parent[rootY] = rootX\n",
    "                    self.rank[rootX] += 1\n",
    "                    self.size[rootX] += self.size[rootY]\n",
    "\n",
    "        def get_size(self, x):\n",
    "            rootX = self.find(x)\n",
    "            return self.size[rootX]\n",
    "\n",
    "    map_merge = [0]*len(clusters)\n",
    "    row_merge = len(clusters)\n",
    "    batch_simi = [[0] * row_merge for _ in range(row_merge)]\n",
    "    for i in range(row_merge):\n",
    "        for j in range(i,row_merge):\n",
    "            batch_simi[i][j] = get_most_simi(clusters[i],clusters[j],simi_matrix)\n",
    "    min_simi = 0.5891500074006119 \n",
    "    new = []\n",
    "    execution_time=0\n",
    "    use_number = 0\n",
    "    total_tokens_call = 0\n",
    "\n",
    "    while True:\n",
    "        the_max_simi_batch = 0\n",
    "        the_first_list = 0\n",
    "        the_second_list = 0\n",
    "        for i in range(row_merge):\n",
    "            for j in range(i,row_merge):\n",
    " \n",
    "                if i==j:\n",
    "                    continue\n",
    "                elif batch_simi[i][j]>the_max_simi_batch:\n",
    "                    the_max_simi_batch = batch_simi[i][j]\n",
    "                    the_first_list = i\n",
    "                    the_second_list = j\n",
    "        if the_max_simi_batch < min_simi:\n",
    "            break\n",
    "        if batch_simi[the_first_list][the_second_list] > 0.6:\n",
    "            batch_simi[the_first_list][the_second_list] = min_simi - 0.01\n",
    "            continue\n",
    "        is_ok = is_act(the_first_list,the_second_list,batch_simi)\n",
    "        print(is_ok)\n",
    "        if is_ok == 1:\n",
    "            batch_simi[the_first_list][the_second_list] = 0\n",
    "        else:\n",
    "            print(batch_simi[the_first_list][the_second_list])\n",
    "            list1 = clusters[the_first_list]\n",
    "            list2 = clusters[the_second_list]\n",
    "            list_all = pick_elements(list1,list2)\n",
    "            prompt = get_prompt_from_ids(list_all, file_path)\n",
    "            start_time = time.time()\n",
    "            completion = client.chat.completions.create(\n",
    "                            model = \"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a worker with rich experience performing Entity Resolution tasks. You specialize in clustering and classification within ER.\"},\n",
    "                            {\"role\": \"user\", \"content\": pre_prompt_merge + prompt},\n",
    "                            ]\n",
    "                        )\n",
    "            execution_time += (time.time() - start_time)\n",
    "            use_number = use_number + 1\n",
    "            token_number = completion.usage.total_tokens\n",
    "            total_tokens_call += token_number\n",
    "            answer = completion.choices[0].message.content.lower().strip() \n",
    "            if 'yes' in answer:\n",
    "                print(\"yes\")\n",
    "                new.append([the_first_list,the_second_list])\n",
    "                map_merge[the_first_list] = 1\n",
    "                map_merge[the_second_list] = 1\n",
    "                batch_simi[the_first_list][the_second_list] = 0\n",
    "            else:\n",
    "                print(\"no\")\n",
    "                batch_simi[the_first_list][the_second_list] =  min_simi-0.01\n",
    "                continue\n",
    "    map_rest = []\n",
    "    for i in range(len(map_merge)):\n",
    "        if map_merge[i] == 0:\n",
    "            map_rest.append(clusters[i])\n",
    "    if new: \n",
    "        with open('./nex_step.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(new)\n",
    "    else:\n",
    "        print(\"No data to write to CSV.\")\n",
    "    matches = pd.read_csv('./nex_step.csv')\n",
    "    column_names = ['ltable_id', 'rtable_id']\n",
    "    matches.columns = column_names\n",
    "    uf = UnionFind()\n",
    "    ids = set()\n",
    "    for _,row in matches.iterrows():\n",
    "        uf.union(row['ltable_id'], row['rtable_id'] )\n",
    "        ids.add(row['ltable_id'])\n",
    "        ids.add(row['rtable_id'])\n",
    "    entity_groups = {}\n",
    "    for _id in ids:\n",
    "        root= uf.find(_id)\n",
    "        if root not in entity_groups:\n",
    "            entity_groups[root] = []\n",
    "        entity_groups[root].append(_id)\n",
    "    result_1 = []\n",
    "    for root, records in entity_groups.items():\n",
    "        result_1.append(records) \n",
    "    result_all_final = result_1+map_rest \n",
    "    print(\"merge done\")\n",
    "    return result_all_final,use_number,execution_time,total_tokens_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'(?<=\\w)([^\\w\\s]+)(?=\\w)', ' ', text)  \n",
    "    text = re.sub(r'(?<!\\w)[^\\w\\s]+(?!\\w)', '', text) \n",
    "    text = re.sub(r'(?<=\\w)[^\\w\\s]+(?!\\w)', '', text) \n",
    "    text = re.sub(r'(?<!\\w)[^\\w\\s]+(?=\\w)', '', text)  \n",
    "    return text.strip().lower() \n",
    "\n",
    "def jaccard_similarity_token(str1, str2):\n",
    "    if len(str1.strip().split()) == 1 and len(str2.strip().split()) == 1 and ('.' in str1 or '.' in str2):\n",
    "        try:\n",
    "            num1 = float(next(iter(str1)))\n",
    "            num2 = float(next(iter(str2)))\n",
    "            match_length = sum(1 for a, b in zip(str1, str2) if a == b)\n",
    "            max_length = max(len(str1), len(str2))\n",
    "            return match_length / max_length\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "    tokens1 = set(preprocess(str1).split())  \n",
    "    tokens2 = set(preprocess(str2).split())\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    union = tokens1.union(tokens2)\n",
    "\n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        return -1.0\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "def calsimi(row_1, row_2, rows):\n",
    "    record1 = rows[row_1]\n",
    "    record2 = rows[row_2]\n",
    "    attributes = set(record1.keys()) - {'id'}\n",
    "    total_weighted_similarity = 0.0\n",
    "    total_weight = 0.0\n",
    "    weights = {attr: 1.0 for attr in attributes}\n",
    "    for attr in attributes:\n",
    "        value1 = str(record1[attr])\n",
    "        value2 = str(record2[attr])\n",
    "        if value1 == 'nan' or value2 == 'nan':\n",
    "            continue\n",
    "        similarity = jaccard_similarity_token(value1, value2)\n",
    "        if similarity != -1.0:\n",
    "            weight = weights[attr]\n",
    "            total_weighted_similarity += similarity * weight\n",
    "            total_weight += weight\n",
    "    if total_weight == 0:\n",
    "        weighted_avg_similarity = 0.0  \n",
    "    else:\n",
    "        weighted_avg_similarity = total_weighted_similarity / total_weight\n",
    "    return weighted_avg_similarity\n",
    "def jaccard_simi(num_i,rows):\n",
    "    ini_simi = [[0.0] * num_i for _ in range(num_i)]\n",
    "    for i in range(num_i):\n",
    "        for j in range(num_i):\n",
    "            ini_simi[i][j] = calsimi(i,j,rows)\n",
    "    return ini_simi\n",
    "\n",
    "def jaccard_block(num_i,ini_simi,block_threshold):\n",
    "    class UnionFind:\n",
    "        def __init__(self):\n",
    "            self.parent = {}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            rootX = self.find(x)\n",
    "            rootY = self.find(y)\n",
    "            if rootX != rootY:\n",
    "                self.parent[rootY] = rootX\n",
    "\n",
    "        def add(self, x):\n",
    "            if x not in self.parent:\n",
    "                self.parent[x] = x\n",
    "\n",
    "    def merge_coordinates(coordinates):\n",
    "        uf = UnionFind()\n",
    "        ids = set()\n",
    "\n",
    "        for ltable_id, rtable_id in coordinates:\n",
    "            uf.add(ltable_id)\n",
    "            uf.add(rtable_id)\n",
    "            uf.union(ltable_id, rtable_id)\n",
    "            ids.add(ltable_id)\n",
    "            ids.add(rtable_id)\n",
    "\n",
    "        entity_groups = {}\n",
    "        for _id in ids:\n",
    "            root = uf.find(_id)\n",
    "            if root not in entity_groups:\n",
    "                entity_groups[root] = []\n",
    "            entity_groups[root].append(_id)\n",
    "\n",
    "        result_1 = []\n",
    "        for root, records in entity_groups.items():\n",
    "            result_1.append(records)\n",
    "        return result_1\n",
    "    \n",
    "    candidate_pair = []\n",
    "    for i in range(num_i):\n",
    "        for j in range(i,num_i):\n",
    "            if ini_simi[i][j] >= block_threshold:\n",
    "                candidate_pair.append([i, j])\n",
    "    uf = UnionFind()\n",
    "    block_result = merge_coordinates(candidate_pair)\n",
    "    return block_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use jacccard block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = './dataset/sigmod/'\n",
    "data_file_path = file_path+'alaska.csv'\n",
    "gt_path = file_path+'alaska_gt.csv'\n",
    "df = pd.read_csv(data_file_path, encoding='MacRoman')\n",
    "rows = df.to_dict(orient='records')\n",
    "block_threshold = 0.7\n",
    "\n",
    "\n",
    "num_of_row, num_of_columns = df.shape\n",
    "block_time = time.time()\n",
    "ini_simi = jaccard_simi(num_of_row,rows)\n",
    "print(\"done\")\n",
    "clusters_block = jaccard_block(num_of_row,ini_simi,block_threshold)\n",
    "execute_block_time = time.time() -  block_time\n",
    "print(f\"execute_jaccard_block_time (s) : {execute_block_time}\")\n",
    "print(len(clusters_block))\n",
    "\n",
    "merge_threshold = 0.5  # the optimal threshold gained by `threshold find function`\n",
    "merge_clusters_pre, use_number, execution_time, total_tokens_call , merge_input_token , merge_output_token = merge_2(clusters_block,ini_simi,data_file_path,block_threshold=block_threshold,merge_threshold=merge_threshold)\n",
    "print(f\"api_call_time_merge: {use_number}\")\n",
    "print(f\"time_merge (s): {execution_time}\")\n",
    "print(f\"token_merge : {total_tokens_call}\")\n",
    "print(f\"token_merge_input : {merge_input_token}\")\n",
    "print(f\"token_merge_output : {merge_output_token}\")\n",
    "print(len(merge_clusters_pre))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "not_done_any = []\n",
    "merge_done = []\n",
    "for row_line in merge_clusters_pre: \n",
    "    if len(row_line)<=5:\n",
    "        not_done_any.append(row_line)\n",
    "    else:\n",
    "        merge_done.append(row_line)\n",
    "print(len(not_done_any))\n",
    "seperate_threshold = 0.7\n",
    "\n",
    "sperate_result, api_call_time_all ,sperate_time, sperate_token, seperate_input_token , seperate_output_token = seperate_jac(ini_simi,merge_done,data_file_path,seperate_threshold)\n",
    "\n",
    "for row in not_done_any:\n",
    "    sperate_result.append(row)\n",
    "print(f\"api_call_time_seperate: {api_call_time_all}\")\n",
    "print(f\"time_seperate (s): {sperate_time}\")\n",
    "print(f\"token_seperate : {sperate_token}\")\n",
    "print(f\"token_seperate_input : {seperate_input_token}\")\n",
    "print(f\"token_seperate_output : {seperate_output_token}\")\n",
    "\n",
    "\n",
    "true_clusters = get_ground_truth(gt_path)\n",
    "predicted_clusters = clusters_block\n",
    "purity = calculate_purity(true_clusters, predicted_clusters)\n",
    "inverse_purity = calculate_inverse_purity(true_clusters, predicted_clusters)\n",
    "fp_measure = calculate_fp_measure(true_clusters, predicted_clusters)\n",
    "acc = calculate_accuracy(true_clusters, predicted_clusters)\n",
    "ari = calculate_ari(true_clusters, predicted_clusters)\n",
    "print(f\"FP-Measure: {fp_measure}\")\n",
    "print(f\"ACC: {acc}\")\n",
    "print(f\"ARI: {ari}\")\n",
    "\n",
    "with open(file_path+\"JAC_parameters.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(f\"block_thre:{block_threshold} , merge_thre:{merge_threshold} , sep_thre:{seperate_threshold} \\n\")\n",
    "    file.write(f\"api_call_time_merge: {use_number}\\n\")\n",
    "    file.write(f\"time_merge (s): {execution_time}\\n\")\n",
    "    file.write(f\"token_merge : {total_tokens_call}\\n\")\n",
    "    file.write(f\"token_merge_input : {merge_input_token}\\n\")\n",
    "    file.write(f\"token_merge_output : {merge_output_token}\\n\")\n",
    "    file.write(f\"api_call_time_seperate: {api_call_time_all}\\n\")\n",
    "    file.write(f\"time_seperate (s): {sperate_time}\\n\")\n",
    "    file.write(f\"token_seperate : {sperate_token}\\n\")\n",
    "    file.write(f\"token_seperate_input : {seperate_input_token}\\n\")\n",
    "    file.write(f\"token_seperate_output : {seperate_output_token}\\n\")\n",
    "    file.write(f\"FP-Measure: {fp_measure}\\n\")\n",
    "    file.write(f\"ACC: {acc}\\n\")\n",
    "    file.write(f\"ARI: {ari}\\n\")\n",
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfcb51ba6a5898a96229f8cdebac4678b232c747c5fc819b474621f23ba7f45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
